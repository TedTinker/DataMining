---
title: "PSTAT 131 Homework Two"
author: "Lilian Lu and Ted Tinker"
date: "10/24/2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
# install.packages("tidyverse")
# install.packages("tree")
# install.packages("plyr")
# install.packages("randomForest")
# install.packages("class")
# install.packages("rpart")
# install.packages("maptree")
# install.packages("ROCR")

library(tidyverse)
library(tree)
library(plyr)
library(randomForest)
library(class)
library(rpart)
library(maptree)
library(ROCR)

# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5)
options(digits = 4)


## indents are for indenting r code as formatted text
## They may need to be adjusted depending on your OS
# if your output looks odd, increase or decrease indent
indent1 = '    '
indent2 = '        '
indent3 = '            '
```

```{r data_input, include=FALSE}
setwd("/Users/Theodore/Desktop/R_Studio_Stuff/data")
#setwd("/Users/Lilianlu/Documents")
spam <- read_table2("spambase.tab", guess_max=2000)
spam <- spam %>%
mutate(y = factor(y, levels=c(0,1), labels=c("good","spam"))) %>%     # label as factors
mutate_at(.vars=vars(-y), .funs=scale)                                # scale others
summary(spam)                                     # As expected, variables are normalized
```

```{r prelimFuncts, include=FALSE}
calc_error_rate <- function(predicted.value, true.value){             # Calculates error
return(mean(true.value!=predicted.value))
}

records = matrix(NA, nrow=3, ncol=2)                                  # Matrix for results
colnames(records) <- c("train.error","test.error")
rownames(records) <- c("knn","tree","logistic")

set.seed(1)                               # We use the same seed specified in the homework
test.indices = sample(1:nrow(spam), 1000)
spam.train=spam[-test.indices,]
spam.test=spam[test.indices,]             # Separate test data from training

nfold = 10                                # Split training data into ten folds
set.seed(1)
folds = seq.int(nrow(spam.train)) %>%     ## sequential obs ids
cut(breaks = nfold, labels=FALSE) %>%     ## sequential fold ids
sample                                    ## random fold ids

```

\section*{1 Selecting $K$ for $K$ Nearest Neighbors}

After loading the dataset and making folds as directed in the preliminary notes, we define a vector of options for $K$, split $spam.test$ into ten folds, and produce a $do.chunk()$ function.

```{r doChunkKNN}
kvec = c(1, seq(10, 50, length.out=5))

do.chunk <- function(chunkid, folddef, Xdat, Ydat, k){
  train = (folddef!=chunkid)
  Xtr = Xdat[train,]
  Ytr = Ydat[train]
  Xvl = Xdat[!train,]
  Yvl = Ydat[!train]
  ## get classifications for current training chunks
  predYtr = knn(train = Xtr, test = Xtr, cl = Ytr, k = k)
  ## get classifications for current test chunk
  predYvl = knn(train = Xtr, test = Xvl, cl = Ytr, k = k)
  return(data.frame(train.error = calc_error_rate(predYtr, Ytr),
                    val.error = calc_error_rate(predYvl, Yvl)))
}
```

Now we perform cross-validation and print the test and training errors for each value of $K$.

```{r crossVal, cache=TRUE}
set.seed(1) #the randomness here affects our lowest test error
zeroes = c(0,0,0,0,0,0)
temp <- data.frame(TrainError = zeroes, TestError=zeroes)   # Make 2*6 0 matrix for storage
a <- 1

for(k in kvec) {    
  lapply(1:10, function(z) {temp[a,] <<- temp[a,] + .1*
        do.chunk(z,folds,spam.train[,-58],spam.train$y,k)})   
        # This runs veeerry slowly. I used the <<- operator to assign outside the scope.
  a = a + 1}
print(temp)
```

$k=10$ seems to yield the lowest test error. 

\clearpage
\section*{2 Training and Test Errors}

Using the value of $k$ chosen in part 1, we calculate the true test error:

```{r recordsKNN, warning=FALSE}
best.kfold=10
YPred = knn(train = spam.train[,-58], test = spam.train[,-58], cl = spam.train$y, k = best.kfold) 
YTest=knn(train = spam.train[,-58],test=spam.test[,-58],cl=spam.train$y,k=best.kfold)
records[1,] <- c(calc_error_rate(YPred,spam.train$y),calc_error_rate(YTest,spam.test$y)) 
print(records)
```




\section*{3 Controlling Decision Tree Construction}

```{r spamtree}
spamtree <- tree(y~.,spam.train,control=tree.control(nrow(spam.train),
                    mincut=1,minsize=5,mindev=1e-5),method="recursive.partition")
summary(spamtree)
```
The tree generated has 184 terminal nodes, or leaves. Of 3601 observations, only 19 are misclassified.



\section*{4 Decision Tree Pruning}

```{r pruned,fig.width=12, fig.height=10}
pruned <- prune.tree(spamtree,best=10)    # Prune to 10 leaves
draw.tree(pruned,nodeinfo=TRUE)           # Draw
```

I notice the pair of leaves on the far left are both good. That split branch hardly seems necessary, but the leftmost leaf is slightly purer in content. 

\clearpage
\section*{5 Pruning Part Two}
```{r pruning2, fig.height=4}
Prunedtr<-cv.tree(spamtree,rand=folds,K=10,method="misclass")        # Make trees
plot(Prunedtr$size,Prunedtr$dev,xlab="Number of Leaves",ylab="Deviation/Misclassification") 
              # Plot leaf data
best.size.cv<-min(Prunedtr$size[which(Prunedtr$dev==min(Prunedtr$dev))])
abline(v=best.size.cv) # Add verticle line at best size
```

The most effective tree-size in this case is 37 leaves, which is the smallest tree minimizing the misclassification error. 

\section*{6a Training and Test Errors}
```{r trainingandtesterror}
spamtree.pruned = prune.tree(spamtree,best=best.size.cv)         # Make tree with 37 leaves
YPred = predict(spamtree.pruned, spam.train, type="class")       # Predict training values
YTest = predict(spamtree.pruned, spam.test,type = "class")       # Predict test values
records[2,] <- c(calc_error_rate(YPred,spam.train$y),calc_error_rate(YTest,spam.test$y)) 
print(records)
```

\clearpage
\section*{6b Show the Inverse of the Logistic Function is the Logit Function}

To show that $\frac{e^z}{1+e^z}$ is the inverse function of $ln\left( \frac{p}{1-p} \right)$, and vice versa, consider the composition  of functions

$$ln\left( \frac{\frac{e^z}{1+e^z}}{1-\frac{e^z}{1+e^z}} \right)$$

By the rules for logorithms of fractions, this composition is equal to

$$ln\left( \frac{e^z}{1+e^z} \right) - ln\left( \frac{1}{1+e^z} \right) $$

Continuting to expand, we find

$$ln\left( e^z \right) - ln\left( 1 + e^z \right)  + ln\left( 1 + e^z \right) = ln\left(e^z\right) = z.$$
Composing the functions returns the argument $z$ to its original state, so they must be inverses of one-another.


\section*{7 Logistic Regression}

```{r logisticfit, warning=FALSE}
logpre<-glm(y~., data=spam.train, family=binomial)    # Use GLM to model the training data binomially
summary(logpre)
```
Here is the summary of our logistic function. Not all the predictors are significant for our logistic model.

```{r logpred}
prob.training = predict(logpre,spam.train,type="response")        # Predict training values
prob.test=predict(logpre,spam.test, type="response")              # Predict test values
predtrain=as.factor(ifelse(prob.training<=0.5, "good", "spam"))   # Classify as good or spam 
predtest=as.factor(ifelse(prob.test<=0.5,"good","spam"))          # at a threshold of 50%
records[3,] <- c(calc_error_rate(predtrain,spam.train$y),calc_error_rate(predtest,spam.test$y)) 
print(records)
```
After testing all three methods, it turns out that the decision tree produces the lowest test error.


\section*{8 Receiver Operating Characteristic Curve}
```{r receiver,fig.height=4.5}
pred.prune = predict(spamtree.pruned, spam.test, type="vector") # Predict test values with tree
prob.test=predict(logpre,spam.test, type="response")            # Predict test valyes with logistic reg
pred1<-prediction(pred.prune[,2],spam.test$y)                   # Test Predictions
pred2<-prediction(prob.test,as.numeric(spam.test$y))
perftree= performance(pred1, measure="tpr", x.measure="fpr")    # Measaure performance of tree
perflog=performance(pred2,measure = "tpr",x.measure = "fpr")    # Measure performance of logreg
plot(perftree, col="red", lwd=2, main="ROC curves")
plot(perflog,col="blue",lwd=2.5,add=TRUE)                       # Plot FPR vs TPR
legend(.5,.2, c("Tree ROC","Log Reg ROC"), lty=c(1,1),lwd=c(2.5,2.5),col=c("red","blue"))
```


The red line represents the decision tree model; the blue line represents the logistic regression model. 

\clearpage
```{r AUC}
auctree = performance(pred1, "auc")@y.values
auclog=performance(pred2,"auc")@y.values
print(auctree)
print(auclog)
```
Since the AUC of logistic regression is larger than the AUC of the decision tree, we consider the performance of logistic regression to be better. 

\section*{9 False Positives VS True Positives}

Regarding spam, I am most worried about false positives, meaning emails are marked as spam when they are actually important. If the false positive rate is too high, an important memo might fly over my head. With a low true positive rate, I'll have to sort spam from my emails by hand, which is just an inconvenience rather than a career-ending mistake. 






